<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>org.nexus</groupId>
    <artifactId>spark-fundamentals</artifactId>
    <version>1.0-SNAPSHOT</version>
    <name>${project.artifactId}</name>

    <description>Spark Fundamentals</description>
    <inceptionYear>2020</inceptionYear>

    <properties>
        <maven.compiler.source>1.8</maven.compiler.source>
        <maven.compiler.target>1.8</maven.compiler.target>
        <encoding>UTF-8</encoding>

        <!-- Plugin versions -->
        <scalatest.maven.plugin.version>1.0</scalatest.maven.plugin.version>
        <maven.scala.version>4.3.1</maven.scala.version>

        <scala.version>2.11.12</scala.version>
        <scala.major.version>2.11</scala.major.version>

        <spark.version>2.4.7</spark.version>

        <!-- Logging libraries -->
        <org.slf4j.version>1.7.5</org.slf4j.version>
        <scala.logging.version>3.9.2</scala.logging.version>
        <logback.classic.version>1.2.3</logback.classic.version>

        <!-- Scalatest Version -->
        <scala.test.version>3.0.9</scala.test.version>
        <com.holdenkarau.spark.testing.base.version>2.4.5_0.14.0</com.holdenkarau.spark.testing.base.version>

        <mysql.connector.version>8.0.22</mysql.connector.version>

        <cloud.bigdataoss.gcs.connector.version>hadoop2-2.2.0</cloud.bigdataoss.gcs.connector.version>
        <hadoop.mapreduce.version>2.9.2</hadoop.mapreduce.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
        </dependency>

        <!-- logging libraries -->
        <!-- https://mvnrepository.com/artifact/ch.qos.logback/logback-classic -->
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-classic</artifactId>
            <version>${logback.classic.version}</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/com.typesafe.scala-logging/scala-logging -->
        <dependency>
            <groupId>com.typesafe.scala-logging</groupId>
            <artifactId>scala-logging_${scala.major.version}</artifactId>
            <version>${scala.logging.version}</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.major.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.major.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive -->
        <!-- Need this dependency for leveraging the spark-testing-base library -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.major.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
            </exclusions>
        </dependency>

        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->
<!--        <dependency>-->
<!--            <groupId>org.apache.spark</groupId>-->
<!--            <artifactId>spark-streaming_${scala.major.version}</artifactId>-->
<!--            <version>${spark.version}</version>-->
<!--            <scope>provided</scope>-->
<!--            <exclusions>-->
<!--                <exclusion>-->
<!--                    <groupId>org.slf4j</groupId>-->
<!--                    <artifactId>slf4j-log4j12</artifactId>-->
<!--                </exclusion>-->
<!--            </exclusions>-->
<!--        </dependency>-->

        <!--        <dependency>-->
        <!--            <groupId>org.apache.hive</groupId>-->
        <!--            <artifactId>hive-jdbc</artifactId>-->
        <!--            <version>1.1.1</version>-->
        <!--            <scope>provided</scope>-->
        <!--        </dependency>-->

        <!--        <dependency>-->
        <!--            <groupId>org.apache.spark</groupId>-->
        <!--            <artifactId>spark-catalyst_${scala.compat.version}</artifactId>-->
        <!--            <version>${spark.version}</version>-->
        <!--        </dependency>-->

        <!--        <dependency>-->
        <!--            <groupId>org.scala-lang.modules</groupId>-->
        <!--            <artifactId>scala-xml_${scala.compat.version}</artifactId>-->
        <!--            <version>1.1.1</version>-->
        <!--        </dependency>-->

        <!--        <dependency>-->
        <!--            <groupId>org.scala-lang.modules</groupId>-->
        <!--            <artifactId>scala-parser-combinators_${scala.compat.version}</artifactId>-->
        <!--            <version>1.1.1</version>-->
        <!--        </dependency>-->

        <!--        <dependency>-->
        <!--            <groupId>org.scala-lang.modules</groupId>-->
        <!--            <artifactId>scala-swing_${scala.compat.version}</artifactId>-->
        <!--            <version>1.0.2</version>-->
        <!--        </dependency>-->

        <!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java -->
        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>${mysql.connector.version}</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/com.google.cloud.bigdataoss/gcs-connector -->
        <dependency>
            <groupId>com.google.cloud.bigdataoss</groupId>
            <artifactId>gcs-connector</artifactId>
            <version>${cloud.bigdataoss.gcs.connector.version}</version>
            <scope>provided</scope>
        </dependency>

        <!-- reference : https://stackoverflow.com/questions/36427291/illegalaccesserror-to-guavas-stopwatch-from-org-apache-hadoop-mapreduce-lib-inp-->
        <!-- Adding following jars explicitly to fix issue with using GCS location in Spark Local run -->
        <!-- https://stackoverflow.com/questions/36427291/illegalaccesserror-to-guavas-stopwatch-from-org-apache-hadoop-mapreduce-lib-inp -->
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-mapreduce-client-core</artifactId>
            <version>[${hadoop.mapreduce.version}]</version>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>slf4j-log4j12</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
            </exclusions>
        </dependency>

        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>[${hadoop.mapreduce.version}]</version>
            <scope>provided</scope>
            <exclusions>
                <exclusion>
                    <artifactId>slf4j-api</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>slf4j-log4j12</artifactId>
                    <groupId>org.slf4j</groupId>
                </exclusion>
            </exclusions>
        </dependency>

        <!--        <dependency>-->
        <!--            <groupId>commons-io</groupId>-->
        <!--            <artifactId>commons-io</artifactId>-->
        <!--            <version>[2.4]</version>-->
        <!--        </dependency>-->

        <!-- Working with Alluxio Mount path-->
        <dependency>
            <groupId>org.alluxio</groupId>
            <artifactId>alluxio-shaded-client</artifactId>
            <version>2.4.0</version>
            <scope>provided</scope>
        </dependency>

        <!-- Testing Framework -->
        <dependency>
            <groupId>org.scalactic</groupId>
            <artifactId>scalactic_${scala.major.version}</artifactId>
            <version>${scala.test.version}</version>
            <scope>test</scope>
        </dependency>

        <dependency>
            <groupId>org.scalatest</groupId>
            <artifactId>scalatest_${scala.major.version}</artifactId>
            <version>${scala.test.version}</version>
            <scope>test</scope>
        </dependency>

        <!-- https://mvnrepository.com/artifact/com.holdenkarau/spark-testing-base -->
        <dependency>
            <groupId>com.holdenkarau</groupId>
            <artifactId>spark-testing-base_${scala.major.version}</artifactId>
            <version>${com.holdenkarau.spark.testing.base.version}</version>
            <scope>test</scope>
            <exclusions>
                <exclusion>
                    <artifactId>hadoop-client</artifactId>
                    <groupId>org.apache.hadoop</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>hadoop-common</artifactId>
                    <groupId>org.apache.hadoop</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>hadoop-hdfs</artifactId>
                    <groupId>org.apache.hadoop</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
                    <groupId>org.apache.hadoop</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>hadoop-minicluster</artifactId>
                    <groupId>org.apache.hadoop</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>hadoop-yarn-server-tests</artifactId>
                    <groupId>org.apache.hadoop</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>hadoop-yarn-server-web-proxy</artifactId>
                    <groupId>org.apache.hadoop</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>scala-library</artifactId>
                    <groupId>org.scala-lang</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>scalatest_2.11</artifactId>
                    <groupId>org.scalatest</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>spark-catalyst_2.11</artifactId>
                    <groupId>org.apache.spark</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>spark-core_2.11</artifactId>
                    <groupId>org.apache.spark</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>spark-hive_2.11</artifactId>
                    <groupId>org.apache.spark</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>spark-mllib_2.11</artifactId>
                    <groupId>org.apache.spark</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>spark-sql_2.11</artifactId>
                    <groupId>org.apache.spark</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>spark-streaming_2.11</artifactId>
                    <groupId>org.apache.spark</groupId>
                </exclusion>
                <exclusion>
                    <artifactId>spark-yarn_2.11</artifactId>
                    <groupId>org.apache.spark</groupId>
                </exclusion>
            </exclusions>
        </dependency>

    </dependencies>

    <build>
        <sourceDirectory>src/main/scala</sourceDirectory>
        <plugins>
            <plugin>
                <!-- see http://davidb.github.com/scala-maven-plugin -->
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>${maven.scala.version}</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                        </goals>
                        <configuration>
                            <args>
                                <arg>-dependencyfile</arg>
                                <arg>${project.build.directory}/.scala_dependencies</arg>
                            </args>
                        </configuration>
                    </execution>
                </executions>
            </plugin>

            <!-- enable scalatest -->
            <plugin>
                <groupId>org.scalatest</groupId>
                <artifactId>scalatest-maven-plugin</artifactId>
                <version>${scalatest.maven.plugin.version}</version>
                <configuration>
                    <tagsToExclude>org.nexus.LocalOnlyTests</tagsToExclude>
                </configuration>

                <executions>
                    <execution>
                        <id>test</id>
                        <goals>
                            <goal>test</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
