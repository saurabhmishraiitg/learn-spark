{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scala Spark Wordcount Example (using DataFrame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "//Using SparkContext, which is already present in the environment\n",
        "val schema = \"col1 String\"\n",
        "\n",
        "val df_inp = spark.read.schema(schema).csv(\"/user/sxxx/data/test01\")\n",
        "df_inp.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val input_file = System.getProperty(\"user.dir\")+\"/../../data/wordcount-input.txt\"\n",
        "// print(input_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "// Read the file into a DataFrame\n",
        "val schema = \"col1 string\"\n",
        "val input_df = spark.read.schema(schema).option(\"delimiter\", \"|\").csv(input_file)\n",
        "input_df.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "// All these packages are pre-imported in the kernel, but listing them here for knowledge sake\n",
        "import org.apache.spark.sql.SparkSession\n",
        "//import spark.implicits._\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.expressions.Window\n",
        "\n",
        "// Split words on each line by space\n",
        "val df_line_split = input_df.withColumn(\"split_line\", split(input_df(\"col1\"), \" \"))\n",
        "\n",
        "// Explode each split line array into individual words\n",
        "val df_word_explode = df_line_split.withColumn(\"word\", explode(df_line_split(\"split_line\")))\n",
        "                                        .drop(\"col1\", \"split_line\")\n",
        "\n",
        "df_word_explode.show(2)\n",
        "\n",
        "// Aggregate on words to get their respective counts\n",
        "val df_word_count = df_word_explode.groupBy(\"word\").agg(count(\"word\").alias(\"cnt\"))\n",
        "                                    .withColumn(\"dummy\", lit(\"1\"))\n",
        "\n",
        "df_word_count.show(5)\n",
        "\n",
        "// Define windowing function\n",
        "val windw = Window.partitionBy(\"dummy\").orderBy(desc(\"cnt\"))\n",
        "\n",
        "val df_word_rank = df_word_count.withColumn(\"rank\", rank().over(windw)).drop(\"dummy\")\n",
        "\n",
        "df_word_rank.show(3)\n",
        "\n",
        "df_word_rank.filter(df_word_rank(\"rank\") === 5).show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "python",
      "pygments_lexer": "scala",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
