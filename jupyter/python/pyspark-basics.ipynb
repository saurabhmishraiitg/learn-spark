{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Description\n",
        "Basic examples of pyspark code in action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using pip\n",
        "\n",
        "- [Reference](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
        "- Create a new  `conda` environment\n",
        "\n",
        "  ```bash\n",
        "  conda create --name pyspark python=3.7\n",
        "  conda activate pyspark\n",
        "  conda deactivate\n",
        "  ```\n",
        "\n",
        "- Install specific `pyspark` version\n",
        "  - `pip index versions pyspark`\n",
        "  - `pip install pyspark==2.4.8`\n",
        "  - `conda install -c conda-forge pyspark`\n",
        "- Install addons\n",
        "  - Spark SQL\n",
        "    - `pip install pyspark[sql]`\n",
        "  - Pandas API with plotly\n",
        "    - `pip install pyspark[pandas_on_spark] plotly`\n",
        "  - To work with Jupyter notebooks\n",
        "    - `conda install -c conda-forge --name pyspark ipykernel -y`\n",
        "  - If want to use local spark library\n",
        "    - `conda install -c conda-forge findspark -y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('PySpark')\\\n",
        "    .master(\"local[*]\")\\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 2)\\\n",
        "    .getOrCreate()\n",
        "\n",
        "# If your data volume is small enough then changing the value for shuffle partitions can help improve performance significantly\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Check spark version\n",
        "print(sc.version)\n",
        "print(spark.sparkContext._conf.get(\"spark.sql.shuffle.partitions\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of Spark SQL operation. Create 2 sample dataframe and join on a common key\n",
        "employee = spark.createDataFrame([\"Alice\",\"Bob\",\"John\"], \"string\").toDF(\"name\")\n",
        "department = spark.createDataFrame([(\"Alice\", \"Finance\"),(\"Bob\", \"HR\"),(\"John\", \"IT\")]).toDF(\"name\", \"department\")\n",
        "\n",
        "employee.join(department, \"name\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reading JSON data\n",
        "import os\n",
        "jsonpath = os.environ[\"HOME\"]+\"/Desktop/tmp/training/spark-advanced/datasets/spark-ml/spark-ml-input.json\"\n",
        "\n",
        "inputdf = spark.read.json(jsonpath)\n",
        "inputdf.show(2)\n",
        "\n",
        "# print(inputdf.rdd.getNumPartitions())\n",
        "\n",
        "# repartitiondf = inputdf.repartition(5)\n",
        "# print(repartitiondf.rdd.getNumPartitions())\n",
        "\n",
        "# print(inputdf.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading Sample data\n",
        "shopping_data = \\\n",
        "[('Alex','2018-10-10','Paint',80),('Alex','2018-04-02','Ladder',20),('Alex','2018-06-22','Stool',20),\\\n",
        "('Alex','2018-12-09','Vacuum',40),('Alex','2018-07-12','Bucket',5),('Alex','2018-02-18','Gloves',5),\\\n",
        "('Alex','2018-03-03','Brushes',30),('Alex','2018-09-26','Sandpaper',10)]\n",
        "\n",
        "from pyspark.sql.types import DateType\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "df = spark.createDataFrame(shopping_data, ['name','date','product','price']).withColumn('date',F.col('date').cast(DateType()))\n",
        "\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic aggregations\n",
        "from pyspark.sql.functions import count, sum\n",
        "\n",
        "df.agg(\\\n",
        "    count('*').alias('cnt'), \\\n",
        "    sum('price').alias('sm')\\\n",
        "    ).show()\n",
        "\n",
        "# Group by aggregations\n",
        "inputdf.groupBy('color')\\\n",
        "    .agg(count('*').alias('cnt')\\\n",
        "    ).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selecting frame in a windowing function\n",
        "# reference : https://towardsdatascience.com/spark-sql-102-aggregations-and-window-functions-9f829eaa7549\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "w_1 = Window.partitionBy('name').orderBy('date')\n",
        "w_2 = Window.partitionBy('name').orderBy('date').rowsBetween(-1, Window.currentRow) #currentRow can be substituted with '0' as well\n",
        "\n",
        "# Adding current row and all the ones before it\n",
        "df.withColumn('sum_1', sum('price').over(w_1)).show()\n",
        "\n",
        "# Adding current row and 1 before\n",
        "df.withColumn('sum_1', sum('price').over(w_2)).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selecting frame continued\n",
        "w_3 = Window.partitionBy('name').orderBy('date').rowsBetween(Window.unboundedPreceding, 0) #this is equivalent to w_1\n",
        "w_4 = Window.partitionBy('name').orderBy('date').rowsBetween(0, Window.unboundedFollowing) #this is opposite of w_3\n",
        "\n",
        "df.withColumn('sum_1', sum('price').over(w_3)).show()\n",
        "\n",
        "df.withColumn('sum_1', sum('price').over(w_4)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a window\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "w0 = Window.partitionBy('name')\n",
        "\n",
        "# Rank vs Dense Rank\n",
        "df.withColumn('price_rank',\\\n",
        "    F.dense_rank()\\\n",
        "    .over(w0.orderBy(F.col('price')\\\n",
        "    .desc())))\\\n",
        "    .show()\n",
        "\n",
        "\n",
        "df.withColumn('price_rank',\\\n",
        "    F.rank()\\\n",
        "    .over(w0.orderBy(F.col('price')\\\n",
        "    .desc())))\\\n",
        "    .show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribute the data into buckets\n",
        "df.withColumn('price_bucket',\\\n",
        "    F.ntile(4)\\\n",
        "    .over(w0.orderBy(F.col('price')\\\n",
        "    .desc())))\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using lead and lag functions\n",
        "df.withColumn('days_since_last', \\\n",
        "        F.datediff('date', F.lag('date', 1)\\\n",
        "            .over(w0.orderBy(F.col('date')))\\\n",
        "        )\n",
        "    )\\\n",
        "    .withColumn('days_before_next', \\\n",
        "        F.datediff(F.lead('date', 1)\\\n",
        "            .over(w0.orderBy(F.col('date')))\\\n",
        "        , 'date'\n",
        "        )\n",
        "    )\\\n",
        "    .show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using collect_set, collect_list\n",
        "df.withColumn('products'\\\n",
        "    , F.collect_set('product')\\\n",
        "        .over(w0.partitionBy('price'))\n",
        "    ).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate moving average of sum over last 30 days\n",
        "days = lambda i: i * 86400 # 86400 seconds in a day  \n",
        "\n",
        "df.withColumn('unix_time',F.col('date').cast('timestamp').cast('long'))\\\n",
        "    .withColumn('moving_avg', \\\n",
        "        F.avg('price')\\\n",
        "            .over(w0.orderBy(F.col('unix_time')).rangeBetween(-days(30), 0)\\\n",
        "        )\\\n",
        "    )\\\n",
        "    .withColumn('days_since_last', \\\n",
        "        F.datediff('date', F.lag('date', 1)\\\n",
        "            .over(w0.orderBy(F.col('date')))\\\n",
        "        )\n",
        "    ).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trend Analysis Example\n",
        "import os\n",
        "csvdf1 = spark.read\\\n",
        "    .options(header = True, inferSchema = True, delimiter = ',', dateFormat = 'MM/dd/yyyy')\\\n",
        "    .csv(os.environ['HOME']+\"/tmp/sample-dataset/salary.csv\")\n",
        "\n",
        "csvdf1.show(2)\n",
        "csvdf1.printSchema()\n",
        "\n",
        "#Parse the date\n",
        "csvdf2 = csvdf1.withColumn('saldt_parse', F.to_date('saldt', format='MM/dd/yyyy'))\n",
        "\n",
        "csvdf2.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "wd = Window.partitionBy('empname').orderBy(F.col('saldt_parse').asc())\n",
        "\n",
        "csvdf2.withColumn(\\\n",
        "    'trend',  \\\n",
        "         F.when(\\\n",
        "             (F.col('salary') - F.coalesce(F.lag('salary', 1).over(wd), F.lit(0))) > 0, 'UP')\\\n",
        "         .otherwise('DOWN')\n",
        ")\\\n",
        ".withColumn(\\\n",
        "    'diff',  \\\n",
        "            (F.col('salary') - F.coalesce(F.lag('salary', 1).over(wd), F.lit(0)))\\\n",
        ").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO Streaming in PySpark"
      ]
    }
  ],
  "metadata": {
    "file_extension": ".py",
    "interpreter": {
      "hash": "2c9fb072f613edaca6cc3b2b8ec9254cad5a4f389ef8b3da889f7e89fb69998e"
    },
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit ('spark': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "orig_nbformat": 2,
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
