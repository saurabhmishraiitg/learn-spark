{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37564bitsparkconda4e3b4d5f3c4d4cadbf8fd6a78e253cc0",
   "display_name": "Python 3.7.5 64-bit ('spark': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "Basic examples of pyspark code in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: findspark in /Users/s0m0158/anaconda3/envs/spark/lib/python3.7/site-packages (1.3.0)\n"
    }
   ],
   "source": [
    "# Import necessary liraries\n",
    "# Use kernel 'spark'\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the findspark library to find the instance of local installed pySpark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/usr/local/opt/apache-spark/libexec\n/Library/Java/JavaVirtualMachines/jdk1.8.0_221.jdk/Contents/Home\n/Users/s0m0158/anaconda3/envs/spark/bin:/Users/s0m0158/anaconda3/condabin:/usr/local/bin/bash:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/s0m0158/Desktop/utils/platform-tools:/Users/s0m0158/maven/latest/bin:/Users/s0m0158/Desktop/repos/rand/leiningen/bin:/Users/s0m0158/Library/Python/2.7/bin:/usr/local/opt/scala@2.11/bin\n"
    }
   ],
   "source": [
    "# Check Spark version\n",
    "import os\n",
    "print(os.environ['SPARK_HOME'])\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Nexus CCM\n",
    "import nexus_ccm as ccm\n",
    "\n",
    "# To ensure refresh of imported module, adding explicit reloading\n",
    "from importlib import reload\n",
    "reload(ccm)\n",
    "\n",
    "#1 time load the CCM. Post that, you can use the \n",
    "ccm.load_ccm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get spark context\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('wordcount').master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement WordCount example in pyspark\n",
    "# Sample data location\n"
   ]
  }
 ]
}