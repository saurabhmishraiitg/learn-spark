{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "Basic examples of pyspark code in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pip\n",
    "\n",
    "- [Reference](https://spark.apache.org/docs/latest/api/python/getting_started/install.html)\n",
    "- Create a new  `conda` environment\n",
    "\n",
    "  ```bash\n",
    "  conda create --name pyspark python=3.7\n",
    "  conda activate pyspark\n",
    "  conda deactivate\n",
    "  ```\n",
    "\n",
    "- Install specific `pyspark` version\n",
    "  - `pip index versions pyspark`\n",
    "  - `pip install pyspark==2.4.8`\n",
    "  - `conda install -c conda-forge pyspark`\n",
    "- Install addons\n",
    "  - Spark SQL\n",
    "    - `pip install pyspark[sql]`\n",
    "  - Pandas API with plotly\n",
    "    - `pip install pyspark[pandas_on_spark] plotly`\n",
    "  - To work with Jupyter notebooks\n",
    "    - `conda install -c conda-forge --name pyspark ipykernel -y`\n",
    "  - If want to use local spark library\n",
    "    - `conda install -c conda-forge findspark -y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('PySpark')\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 2)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# If your data volume is small enough then changing the value for shuffle partitions can help improve performance significantly\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Check spark version\n",
    "print(sc.version)\n",
    "print(spark.sparkContext._conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Spark SQL operation. Create 2 sample dataframe and join on a common key\n",
    "employee = spark.createDataFrame([\"Alice\",\"Bob\",\"John\"], \"string\").toDF(\"name\")\n",
    "department = spark.createDataFrame([(\"Alice\", \"Finance\"),(\"Bob\", \"HR\"),(\"John\", \"IT\")]).toDF(\"name\", \"department\")\n",
    "\n",
    "employee.join(department, \"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading JSON data\n",
    "import os\n",
    "jsonpath = os.environ[\"HOME\"]+\"/Desktop/tmp/training/spark-advanced/datasets/spark-ml/spark-ml-input.json\"\n",
    "\n",
    "inputdf = spark.read.json(jsonpath)\n",
    "inputdf.show(2)\n",
    "\n",
    "# print(inputdf.rdd.getNumPartitions())\n",
    "\n",
    "# repartitiondf = inputdf.repartition(5)\n",
    "# print(repartitiondf.rdd.getNumPartitions())\n",
    "\n",
    "# print(inputdf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Sample data\n",
    "shopping_data = \\\n",
    "[('Alex','2018-10-10','Paint',80),('Alex','2018-04-02','Ladder',20),('Alex','2018-06-22','Stool',20),\\\n",
    "('Alex','2018-12-09','Vacuum',40),('Alex','2018-07-12','Bucket',5),('Alex','2018-02-18','Gloves',5),\\\n",
    "('Alex','2018-03-03','Brushes',30),('Alex','2018-09-26','Sandpaper',10)]\n",
    "\n",
    "from pyspark.sql.types import DateType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = spark.createDataFrame(shopping_data, ['name','date','product','price']).withColumn('date',F.col('date').cast(DateType()))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic aggregations\n",
    "from pyspark.sql.functions import count, sum\n",
    "\n",
    "df.agg(\\\n",
    "    count('*').alias('cnt'), \\\n",
    "    sum('price').alias('sm')\\\n",
    "    ).show()\n",
    "\n",
    "# Group by aggregations\n",
    "inputdf.groupBy('color')\\\n",
    "    .agg(count('*').alias('cnt')\\\n",
    "    ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting frame in a windowing function\n",
    "# reference : https://towardsdatascience.com/spark-sql-102-aggregations-and-window-functions-9f829eaa7549\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w_1 = Window.partitionBy('name').orderBy('date')\n",
    "w_2 = Window.partitionBy('name').orderBy('date').rowsBetween(-1, Window.currentRow) #currentRow can be substituted with '0' as well\n",
    "\n",
    "# Adding current row and all the ones before it\n",
    "df.withColumn('sum_1', sum('price').over(w_1)).show()\n",
    "\n",
    "# Adding current row and 1 before\n",
    "df.withColumn('sum_1', sum('price').over(w_2)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting frame continued\n",
    "w_3 = Window.partitionBy('name').orderBy('date').rowsBetween(Window.unboundedPreceding, 0) #this is equivalent to w_1\n",
    "w_4 = Window.partitionBy('name').orderBy('date').rowsBetween(0, Window.unboundedFollowing) #this is opposite of w_3\n",
    "\n",
    "df.withColumn('sum_1', sum('price').over(w_3)).show()\n",
    "\n",
    "df.withColumn('sum_1', sum('price').over(w_4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "w0 = Window.partitionBy('name')\n",
    "\n",
    "# Rank vs Dense Rank\n",
    "df.withColumn('price_rank',\\\n",
    "    F.dense_rank()\\\n",
    "    .over(w0.orderBy(F.col('price')\\\n",
    "    .desc())))\\\n",
    "    .show()\n",
    "\n",
    "\n",
    "df.withColumn('price_rank',\\\n",
    "    F.rank()\\\n",
    "    .over(w0.orderBy(F.col('price')\\\n",
    "    .desc())))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+------------+\n",
      "|name|      date|  product|price|price_bucket|\n",
      "+----+----------+---------+-----+------------+\n",
      "|Alex|2018-10-10|    Paint|   80|           1|\n",
      "|Alex|2018-12-09|   Vacuum|   40|           1|\n",
      "|Alex|2018-03-03|  Brushes|   30|           2|\n",
      "|Alex|2018-04-02|   Ladder|   20|           2|\n",
      "|Alex|2018-06-22|    Stool|   20|           3|\n",
      "|Alex|2018-09-26|Sandpaper|   10|           3|\n",
      "|Alex|2018-07-12|   Bucket|    5|           4|\n",
      "|Alex|2018-02-18|   Gloves|    5|           4|\n",
      "+----+----------+---------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Distribute the data into buckets\n",
    "df.withColumn('price_bucket',\\\n",
    "    F.ntile(4)\\\n",
    "    .over(w0.orderBy(F.col('price')\\\n",
    "    .desc())))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+---------------+----------------+\n",
      "|name|      date|  product|price|days_since_last|days_before_next|\n",
      "+----+----------+---------+-----+---------------+----------------+\n",
      "|Alex|2018-02-18|   Gloves|    5|           null|              13|\n",
      "|Alex|2018-03-03|  Brushes|   30|             13|              30|\n",
      "|Alex|2018-04-02|   Ladder|   20|             30|              81|\n",
      "|Alex|2018-06-22|    Stool|   20|             81|              20|\n",
      "|Alex|2018-07-12|   Bucket|    5|             20|              76|\n",
      "|Alex|2018-09-26|Sandpaper|   10|             76|              14|\n",
      "|Alex|2018-10-10|    Paint|   80|             14|              60|\n",
      "|Alex|2018-12-09|   Vacuum|   40|             60|            null|\n",
      "+----+----------+---------+-----+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using lead and lag functions\n",
    "df.withColumn('days_since_last', \\\n",
    "        F.datediff('date', F.lag('date', 1)\\\n",
    "            .over(w0.orderBy(F.col('date')))\\\n",
    "        )\n",
    "    )\\\n",
    "    .withColumn('days_before_next', \\\n",
    "        F.datediff(F.lead('date', 1)\\\n",
    "            .over(w0.orderBy(F.col('date')))\\\n",
    "        , 'date'\n",
    "        )\n",
    "    )\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------------+\n",
      "|name|      date|  product|price|        products|\n",
      "+----+----------+---------+-----+----------------+\n",
      "|Alex|2018-07-12|   Bucket|    5|[Bucket, Gloves]|\n",
      "|Alex|2018-02-18|   Gloves|    5|[Bucket, Gloves]|\n",
      "|Alex|2018-03-03|  Brushes|   30|       [Brushes]|\n",
      "|Alex|2018-09-26|Sandpaper|   10|     [Sandpaper]|\n",
      "|Alex|2018-04-02|   Ladder|   20| [Ladder, Stool]|\n",
      "|Alex|2018-06-22|    Stool|   20| [Ladder, Stool]|\n",
      "|Alex|2018-12-09|   Vacuum|   40|        [Vacuum]|\n",
      "|Alex|2018-10-10|    Paint|   80|         [Paint]|\n",
      "+----+----------+---------+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using collect_set, collect_list\n",
    "df.withColumn('products'\\\n",
    "    , F.collect_set('product')\\\n",
    "        .over(w0.partitionBy('price'))\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+---------+-----+----------+----------+---------------+\n",
      "|name|      date|  product|price| unix_time|moving_avg|days_since_last|\n",
      "+----+----------+---------+-----+----------+----------+---------------+\n",
      "|Alex|2018-02-18|   Gloves|    5|1518892200|       5.0|           null|\n",
      "|Alex|2018-03-03|  Brushes|   30|1520015400|      17.5|             13|\n",
      "|Alex|2018-04-02|   Ladder|   20|1522607400|      25.0|             30|\n",
      "|Alex|2018-06-22|    Stool|   20|1529605800|      20.0|             81|\n",
      "|Alex|2018-07-12|   Bucket|    5|1531333800|      12.5|             20|\n",
      "|Alex|2018-09-26|Sandpaper|   10|1537900200|      10.0|             76|\n",
      "|Alex|2018-10-10|    Paint|   80|1539109800|      45.0|             14|\n",
      "|Alex|2018-12-09|   Vacuum|   40|1544293800|      40.0|             60|\n",
      "+----+----------+---------+-----+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate moving average of sum over last 30 days\n",
    "days = lambda i: i * 86400 # 86400 seconds in a day  \n",
    "\n",
    "df.withColumn('unix_time',F.col('date').cast('timestamp').cast('long'))\\\n",
    "    .withColumn('moving_avg', \\\n",
    "        F.avg('price')\\\n",
    "            .over(w0.orderBy(F.col('unix_time')).rangeBetween(-days(30), 0)\\\n",
    "        )\\\n",
    "    )\\\n",
    "    .withColumn('days_since_last', \\\n",
    "        F.datediff('date', F.lag('date', 1)\\\n",
    "            .over(w0.orderBy(F.col('date')))\\\n",
    "        )\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+-----------+\n",
      "|empid|empname|salary|      saldt|\n",
      "+-----+-------+------+-----------+\n",
      "|    1|   John|  1000|01/01/2016 |\n",
      "|    1|   John|  2000|02/01/2016 |\n",
      "+-----+-------+------+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- empid: integer (nullable = true)\n",
      " |-- empname: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- saldt: string (nullable = true)\n",
      "\n",
      "+-----+-------+------+-----------+-----------+\n",
      "|empid|empname|salary|      saldt|saldt_parse|\n",
      "+-----+-------+------+-----------+-----------+\n",
      "|    1|   John|  1000|01/01/2016 | 2016-01-01|\n",
      "|    1|   John|  2000|02/01/2016 | 2016-02-01|\n",
      "+-----+-------+------+-----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trend Analysis Example\n",
    "import os\n",
    "csvdf1 = spark.read\\\n",
    "    .options(header = True, inferSchema = True, delimiter = ',', dateFormat = 'MM/dd/yyyy')\\\n",
    "    .csv(os.environ['HOME']+\"/tmp/sample-dataset/salary.csv\")\n",
    "\n",
    "csvdf1.show(2)\n",
    "csvdf1.printSchema()\n",
    "\n",
    "#Parse the date\n",
    "csvdf2 = csvdf1.withColumn('saldt_parse', F.to_date('saldt', format='MM/dd/yyyy'))\n",
    "\n",
    "csvdf2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+-----------+-----------+-----+-----+\n",
      "|empid|empname|salary|      saldt|saldt_parse|trend| diff|\n",
      "+-----+-------+------+-----------+-----------+-----+-----+\n",
      "|    1|   John|  1000|01/01/2016 | 2016-01-01|   UP| 1000|\n",
      "|    1|   John|  2000|02/01/2016 | 2016-02-01|   UP| 1000|\n",
      "|    1|   John|  1000|03/01/2016 | 2016-03-01| DOWN|-1000|\n",
      "|    1|   John|  2000|04/01/2016 | 2016-04-01|   UP| 1000|\n",
      "|    1|   John|  3000|05/01/2016 | 2016-05-01|   UP| 1000|\n",
      "|    1|   John|  1000| 06/01/2016| 2016-06-01| DOWN|-2000|\n",
      "+-----+-------+------+-----------+-----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "wd = Window.partitionBy('empname').orderBy(F.col('saldt_parse').asc())\n",
    "\n",
    "csvdf2.withColumn(\\\n",
    "    'trend',  \\\n",
    "         F.when(\\\n",
    "             (F.col('salary') - F.coalesce(F.lag('salary', 1).over(wd), F.lit(0))) > 0, 'UP')\\\n",
    "         .otherwise('DOWN')\n",
    ")\\\n",
    ".withColumn(\\\n",
    "    'diff',  \\\n",
    "            (F.col('salary') - F.coalesce(F.lag('salary', 1).over(wd), F.lit(0)))\\\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Streaming in PySpark"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "interpreter": {
   "hash": "2c9fb072f613edaca6cc3b2b8ec9254cad5a4f389ef8b3da889f7e89fb69998e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('spark': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "orig_nbformat": 2,
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
