{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the kernel specific to SPARK\n",
    "\n",
    "* conda env list\n",
    "* conda activate spark\n",
    "* in visual code IDE, select the environment as python-spark\n",
    "* check that in the selected environment pyspark is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: pyspark\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Package(s) not found: findspark\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 show pyspark\n",
    "!pip3 show findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line help the Jupyter program to find the Spark binaries to run the job\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample program to validate pySpark library is available\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "txt = sc.textFile('file:////Users/sxxx/github/spark-scala/README.md')\n",
    "print(txt.count())\n",
    "\n",
    "python_lines = txt.filter(lambda line: 'python' in line.lower())\n",
    "print(python_lines.count())\n",
    "\n",
    "\n",
    "#big_list = range(10000)\n",
    "#>>> rdd = sc.parallelize(big_list, 2)\n",
    "#>>> odds = rdd.filter(lambda x: x % 2 != 0)\n",
    "#>>> odds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another random code snippet to check if the Spark session is still alive after the previous cell execution\n",
    "big_list = range(10000)\n",
    "rdd = sc.parallelize(big_list, 2)\n",
    "odds = rdd.filter(lambda x: x % 2 != 0)\n",
    "odds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CCM properties file\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "separator = \"=\"\n",
    "keys = {}\n",
    "\n",
    "# I named your file conf and stored it \n",
    "# in the same directory as the script\n",
    "\n",
    "with open(home+'/nexus.prop') as f:\n",
    "\n",
    "    for line in f:\n",
    "        if separator in line:\n",
    "\n",
    "            # Find the name and value by splitting the string\n",
    "            name, value = line.split(separator, 1)\n",
    "\n",
    "            # Assign key value pair to dict\n",
    "            # strip() removes white space from the ends of strings\n",
    "            keys[name.strip()] = value.strip()\n",
    "\n",
    "#print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section handles adding JDBC driver to the PYSPARK shell\n",
    "import os\n",
    "jdbc_connector_mysql=keys[\"jdbc-connector-mysql\"]\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"--jars file://{jdbc_connector_mysql} pyspark-shell\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sparksession instance\n",
    "\n",
    "spark.stop()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch 1 workflow records from the templates table\n",
    "#database = 'reach'\n",
    "#url='jdbc:mysql://{}/{}'.format(host, database)\n",
    "#table = 'reach.tasks'\n",
    "\n",
    "table = \"(SELECT workflowId, REPLACE(CONVERT(REPLACE(workflow,'&amp;','&') using utf8),'','') as workflow FROM reach.templates LIMIT 3) AS t\"\n",
    "\n",
    "url = keys[\"reach-dev-url\"]\n",
    "#query='select taskid from tasks limit 10'\n",
    "user = keys[\"reach-dev-userid\"]\n",
    "password = keys[\"reach-dev-password\"]\n",
    "\n",
    "properties = {\n",
    "    'user': user,\n",
    "    'password': password,\n",
    "    'driver': 'com.mysql.jdbc.Driver',\n",
    "    #'query': 'select taskid from tasks limit 10',\n",
    "    'fetchsize': '10'\n",
    "}\n",
    "\n",
    "df = spark.read.jdbc(url, table, properties=properties)\n",
    "#df = sqlContext.read.jdbc(url, query, properties=properties)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the json column\n",
    "df.select(\"workflow\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import from_json, col\n",
    "#json_schema = spark.read.json(df.select(\"workflow\").rdd.map(lambda row: row.json)).schema\n",
    "#df.withColumn('json', from_json(col('json'), json_schema))\n",
    "\n",
    "workflowRDD = df.select(\"workflow\").rdd\n",
    "\n",
    "workflowRDD.map(lambda r: r.).take(2)\n",
    "#workflowRDD.map(lambda r: r.toJSON).take(2)\n",
    "#workflowRDD.take(2)\n",
    "\n",
    "#new_df = spark.read.json(.map(lambda r: r.json))\n",
    "#new_df.printSchema()\n",
    "\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflowRDD2 = df.select(\"workflow\").toJSON()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflowRDD2.map(lambda r: type(r)).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflowRDD2.map(lambda r: r[1:30]).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = spark.read.json(workflowRDD2).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_schema.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "df.withColumn('json', from_json(col('workflow'), json_schema)).printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflowRDD3 = df.select(\"workflow\").rdd.map(list)\n",
    "workflowRDD3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflowRDD3.map(lambda r: type(r)).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflowRDD4 = workflowRDD3.map(lambda r: r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflowRDD4.map(lambda r: r[1:30]).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema2 = spark.read.json(workflowRDD4).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json_schema2.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "df.withColumn('json', from_json(col('workflow'), json_schema2)).printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=df.withColumn('json', from_json(col('workflow'), json_schema2))\n",
    "new_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "new_df.select(\"json.id\", \"json.description\", explode(\"json.startADGroups\").alias(\"start_adgroup\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = new_df.select(col(\"json.id\").alias(\"id\"), explode(\"json.sections\").alias(\"section\")).select(\"id\", explode(\"section.fields\").alias(\"field\")).select(col(\"field.id\").alias(\"fieldId\")).distinct().limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "new_df3 = new_df2.select(\"fieldId\", lit(1).alias(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "pivotDF = new_df3.groupBy(\"id\").pivot(\"fieldId\").min(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df3.join(pivotDF,on=\"id\", how=\"inner\").show()\n",
    "cross_df = new_df3.crossJoin(pivotDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = cross_df.drop(\"fieldId\", \"id\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_fin_df = cross_df.drop(\"id\").select(*[lit(column).alias(column) if column in column_list else column for column in cross_df.drop(\"id\").columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_fin_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df = pre_fin_df.drop(\"fieldId\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from templates table for workflowid=a611477c-41f1-4a9e-9721-d7afeaf55099\n",
    "#REPLACE(CONVERT(REPLACE(workflow,'&amp;','&') using utf8),'','') as workflow\n",
    "table = \"(SELECT workflowId, workflowVersion, publishStatus, isActive, lastUpdatedTs, lastPublishedTs, REPLACE(CONVERT(REPLACE(lastUpdatedBy,'&amp;','&') using utf8),'','') as lastUpdatedBy, REPLACE(CONVERT(REPLACE(lastPublishedBy,'&amp;','&') using utf8),'','') as lastPublishedBy, REPLACE(CONVERT(REPLACE(createdBy,'&amp;','&') using utf8),'','') as createdBy, createdTs, name, description, REPLACE(CONVERT(REPLACE(workflow,'&amp;','&') using utf8),'','') as workflow FROM reach.templates where workflowid='a611477c-41f1-4a9e-9721-d7afeaf55099' and workflowversion < 11) AS t\"\n",
    "\n",
    "#table = \"(SELECT workflowid, name, count(1) as cnt from reach.templates group by workflowid, name) AS t\"\n",
    "\n",
    "url = keys[\"reach-dev-url\"]\n",
    "user = keys[\"reach-dev-userid\"]\n",
    "password = keys[\"reach-dev-password\"]\n",
    "\n",
    "properties = {\n",
    "    'user': user,\n",
    "    'password': password,\n",
    "    'driver': 'com.mysql.jdbc.Driver',\n",
    "    'fetchsize': '10'\n",
    "}\n",
    "\n",
    "templatesDF = spark.read.jdbc(url, table, properties=properties)\n",
    "templatesDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templatesDF.select(\"workflowid\", \"workflowversion\", \"publishstatus\", \"isactive\", \"lastupdatedts\", \"lastpublishedts\", \"name\", \"description\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten out the workflowMetadata information from templates table\n",
    "#templatesFlatten1 = templatesDF.select(\"workflowid\", \"workflowversion\", \"workflow\")\n",
    "templatesFlatten1 = templatesDF.select(\"workflow\")\n",
    "templatesFlatten2 = templatesFlatten1.rdd.map(list)\n",
    "templatesFlatten2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templatesFlatten3 = templatesFlatten2.map(lambda r : r[0])\n",
    "#templatesFlatten3.map(lambda r : type(r)).take(2)\n",
    "workflow_schema = spark.read.json(templatesFlatten3).schema\n",
    "print(workflow_schema.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, explode\n",
    "templateParsed = templatesFlatten1.withColumn('json', from_json(col('workflow'), workflow_schema))\n",
    "templateParsed.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###****************DEBUG****************###\n",
    "#templateParsed.count()\n",
    "templateParsed.select(col(\"json.id\").alias(\"workflowid\"), col(\"json.version\").alias(\"workflowVersion\"), explode(\"json.sections\").alias(\"section\")).select(\"workflowid\", \"workflowversion\", col(\"section.id\").alias(\"sectionid\")).filter(col(\"workflowversion\")==10).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###****************DEBUG****************###\n",
    "from pyspark.sql.functions import collect_set, sort_array\n",
    "templateSectionsDF = templateParsed.select(col(\"json.id\").alias(\"workflowid\"), col(\"json.version\").alias(\"workflowVersion\"), col(\"json.sections\").alias(\"sections\"))\n",
    "\n",
    "templateSectionExplodedDF = templateSectionsDF.select(\"workflowid\", \"workflowVersion\", explode(\"sections\").alias(\"section\")).select(\"workflowid\", \"workflowversion\", col(\"section.id\").alias(\"sectionid\"), col(\"section.description\").alias(\"sectiondesc\"), col(\"section.order\").alias(\"sectionorder\"), col(\"section.title\").alias(\"sectiontitle\"), col(\"section.type\").alias(\"sectiontype\"), col(\"section.fields\").alias(\"fields\"))\n",
    "\n",
    "templateSectionExplodedDF.groupBy(col(\"workflowid\"), col(\"sectionid\")).agg(sort_array(collect_set(col(\"workflowversion\")))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###****************DEBUG****************###\n",
    "templateSectionExplodedDF.select(\"workflowid\", \"workflowversion\", \"sectionid\", \"sectionorder\", \"sectiontitle\", \"sectiontype\").filter(col(\"workflowversion\").isin({1, 2})).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###****************DEBUG****************###\n",
    "from pyspark.sql.functions import substring\n",
    "templateFieldExplodedDF = templateSectionExplodedDF.select(\"workflowid\", \"workflowversion\", \"sectionid\", \"sectionorder\", \"sectiontitle\", \"sectiontype\", explode(\"fields\").alias(\"field\")).select(\"workflowid\", \"workflowversion\", \"sectionid\", \"sectionorder\", \"sectiontitle\", \"sectiontype\", col(\"field.id\").alias(\"fieldid\"), col(\"field.helpertext\").alias(\"fieldhelpertext\"), col(\"field.hidden\").alias(\"fieldishidden\"), col(\"field.includeyear\").alias(\"fieldincludeyear\"), col(\"field.isfilterable\").alias(\"fieldisfilterable\"), col(\"field.label\").alias(\"fieldlabel\"), col(\"field.options\").alias(\"fieldoptions\"), col(\"field.order\").alias(\"fieldorder\"), col(\"field.placeholder\").alias(\"fieldplaceholder\"), col(\"field.prefix\").alias(\"fieldprefix\"), col(\"field.required\").alias(\"fieldidrequired\"), col(\"field.responses\").alias(\"fieldresponses\"), col(\"field.type\").alias(\"fieldtype\"))\n",
    "\n",
    "#templateFieldExplodedDF.select(col(\"workflowversion\").alias(\"wid\"), substring(\"sectionid\", 0, 3).alias(\"sid\"), col(\"sectionorder\").alias(\"sord\"), substring(\"sectiontitle\",0, 20).alias(\"sttl\"), col(\"sectiontype\").alias(\"styp\"), substring(\"fieldid\", 0, 10).alias(\"fid\"), substring(\"fieldhelpertext\", 0, 15).alias(\"fhlp\"), col(\"fieldishidden\").alias(\"fhid\"), col(\"fieldincludeyear\").alias(\"fiy\"), col(\"fieldisfilterable\").alias(\"fif\"), substring(\"fieldlabel\", 0, 15).alias(\"flbl\"), substring(col(\"fieldoptions\").cast(\"string\"), 0, 30).alias(\"fopt\"), \"fieldorder\", \"fieldidrequired\", \"fieldresponses\", \"fieldtype\").filter(col(\"wid\") == 1).distinct().sort(col(\"sid\"), col(\"fieldorder\")).show(50, truncate=False)\n",
    "\n",
    "templateFieldExplodedDF.select(col(\"workflowversion\").alias(\"wid\"), substring(\"sectionid\", 0, 3).alias(\"sid\"), col(\"sectionorder\").alias(\"sord\"), substring(\"sectiontitle\",0, 50).alias(\"sttl\"), col(\"sectiontype\").alias(\"styp\"), substring(\"fieldid\", 0, 10).alias(\"fid\"), substring(\"fieldlabel\", 0, 15).alias(\"flbl\"), \"fieldorder\", \"fieldtype\").filter(col(\"wid\") == 1).distinct().sort(col(\"sid\"), col(\"fieldorder\")).show(50, truncate=False)\n",
    "#sectiontitle_coalesce(fieldlabel, fieldtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing a single JSON workflow metadata to look at all records in full\n",
    "tmpdir = keys[\"tmp-dir\"]\n",
    "templateParsed.select(\"json\").limit(1).write.mode(\"overwrite\").json(tmpdir + \"/templates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedWorkflowMeta1 = templateParsed.select(col(\"json.id\").alias(\"workflowid\"), col(\"json.version\").alias(\"workflowVersion\"), explode(\"json.sections\").alias(\"section\")).select(\"workflowid\", \"workflowversion\", col(\"section.id\").alias(\"sectionid\"), explode(\"section.fields\").alias(\"field\")).select(\"workflowid\", \"workflowversion\", \"sectionid\", col(\"field.id\").alias(\"fieldId\")).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedWorkflowMeta1.show(truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.functions import udf, StringType, lit, isnull\n",
    "#Create a UDF to transform the sectiontitle, fieldlable, fieldtype columns into a concatenated column\n",
    "def generate_col(sectiontitle, fieldlabel, fieldtype):\n",
    "    #''.join(char for char in sectiontitle if char.isalnum())\n",
    "    sectiontitle_fmt = re.sub('[ ]+', '_', re.sub('[^A-Za-z0-9 ]+', '', sectiontitle)).lower()\n",
    "    \n",
    "    fieldlabel_fmt = ''\n",
    "    if fieldlabel is None:\n",
    "        fieldlabel_fmt = re.sub('[ ]+', '_', re.sub('[^A-Za-z0-9 ]+', '', fieldtype)).lower()\n",
    "    else:\n",
    "        fieldlabel_fmt = re.sub('[ ]+', '_', re.sub('[^A-Za-z0-9 ]+', '', fieldlabel)).lower()\n",
    "\n",
    "    return 'dyn_'+sectiontitle_fmt+'_'+fieldlabel_fmt\n",
    "#print(generate_col(\"Inspector's Contact Information\", \"null\", \"c\"))    \n",
    "\n",
    "\n",
    "generate_col_udf = udf(generate_col, StringType())\n",
    "#templateFieldExplodedDF.withColumn(\"title\", generate_col_udf(col(\"sectiontitle\"), col(\"fieldlabel\"), col(\"fieldtype\")))\\\n",
    "#    .select(\"workflowversion\", \"sectionid\", \"fieldid\", \"fieldorder\", \"title\", \"fieldlabel\")\\\n",
    "#    .filter(col(\"workflowversion\") == 1)\\\n",
    "#    .show(50, truncate=False)\n",
    "\n",
    "#.filter((col(\"workflowversion\") == 1) & (isnull(col(\"fieldlabel\"))))\\\n",
    "\n",
    "workflowTemplateSchema = templateFieldExplodedDF.withColumn(\"title\", generate_col_udf(col(\"sectiontitle\"), col(\"fieldlabel\"), col(\"fieldtype\")))\\\n",
    "    .select(\"workflowid\", \"workflowversion\", \"fieldid\", \"title\")\n",
    "\n",
    "workflowTemplateSchema.filter(col(\"workflowversion\")==1).show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if the fieldId and title are PK combination for a given workflow version\n",
    "from pyspark.sql.functions import countDistinct, count\n",
    "workflowTemplateSchema.groupBy(\"workflowid\", \"workflowversion\")\\\n",
    "    .agg(countDistinct(\"fieldId\"), countDistinct(\"title\"), count(\"title\"))\\\n",
    "    .sort(col(\"workflowversion\"))\\\n",
    "    .show(truncate=False)\n",
    "#(countDistinct(\"fieldid\").alias(\"fieldid_cnt\"), count(col(\"fieldid\")).alias(\"tot_cnt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivoting the workflowTemplateSchema dataframe\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "workflowSchemaPivot = workflowTemplateSchema\\\n",
    "    .groupBy(\"workflowid\", \"workflowversion\")\\\n",
    "    .pivot(\"title\")\\\n",
    "    .agg(first(\"fieldid\"))\\\n",
    "    .sort(\"workflowversion\")\n",
    "\n",
    "workflowSchemaPivot.select(\"workflowid\", 'workflowversion', 'dyn_observation_status','dyn_observation_dependentdropdownlist').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe for workflow attributes\n",
    "templatesNonSchemaAttribute = templatesDF.select(\"workflowid\", \"workflowversion\", col(\"name\").alias(\"workflowname\"), col(\"description\").alias(\"workflowdescription\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from submissions table for workflowid='a0ab07fa-fe40-4eb5-bdef-5b505defd91a'\n",
    "#table = \"(SELECT workflowid, workflowversion, count(1) as cnt from reach.submissions where workflowid='a611477c-41f1-4a9e-9721-d7afeaf55099' group by workflowid, workflowversion) AS t\"\n",
    "table = \"(SELECT submissionid, currentstep, totalsteps, workflowid, workflowversion, createdts, lastupdatedts, createdby, lastupdatedname, createdbyname, recordid, countrycode, REPLACE(CONVERT(REPLACE(stepmetadata,'&amp;','&') using utf8),'','') as stepmetadata, tzoffset from reach.submissions where workflowid='a611477c-41f1-4a9e-9721-d7afeaf55099' and workflowversion<11) AS t\"\n",
    "\n",
    "submissionsDF = spark.read.jdbc(url, table, properties=properties)\n",
    "submissionsDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissionsDF.select(\"submissionid\", col(\"currentstep\").alias(\"submissionCurrentStep\"), \"totalsteps\", \"workflowid\", \"workflowversion\", col(\"createdts\").alias(\"submissionCreatedTs\"), col(\"lastupdatedts\").alias(\"submissionLastUpdatedTs\"), \"recordid\", \"countrycode\", \"tzoffset\").show(11, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissionAttribute = submissionsDF.select(\"submissionid\", col(\"currentstep\").alias(\"submissionCurrentStep\"), col(\"workflowid\").alias(\"submissionWorkflowId\"), col(\"workflowversion\").alias(\"submissionWorkflowVersion\"), col(\"createdts\").alias(\"submissionCreatedTs\"), col(\"lastupdatedts\").alias(\"submissionLastUpdatedTs\"), \"recordid\", col(\"countrycode\").alias(\"submissionCountryCode\"))\n",
    "\n",
    "submissionAttribute.show(truncate=False)\n",
    "submissionAttribute.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from responses table for workflowid='a0ab07fa-fe40-4eb5-bdef-5b505defd91a'\n",
    "#table = \"(SELECT workflowid, count(1) as cnt from reach.responses group by workflowid) AS t\"\n",
    "table = \"(SELECT responseid, submissionid, fieldid, submittedby, submitter, lastupdatedts, siteid, REPLACE(CONVERT(REPLACE(value,'&amp;','&') using utf8),'','') as value, `order`, currentstep, createdTs, submittedByName, lastUpdatedBy, taskid from reach.responses where workflowid='a611477c-41f1-4a9e-9721-d7afeaf55099' and workflowVersion<11) AS t\"\n",
    "\n",
    "responsesDF = spark.read.jdbc(url, table, properties=properties)\n",
    "responsesDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsesDF.select(\"responseid\", \"submissionid\", \"fieldid\", \"lastupdatedts\", \"siteid\", col(\"value\").alias(\"responseValue\"), \"order\", \"currentstep\", \"taskid\").show(truncate=False)\n",
    "responsesDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responseAttribute = responsesDF.select(\"responseid\", col(\"submissionid\").alias(\"responseSubmissionId\"), \"fieldid\", col(\"value\").alias(\"responseValue\"), col(\"order\").alias(\"responseOrder\"), col(\"taskid\").alias(\"responseTaskId\"))\n",
    "responseAttribute.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from tasks table for workflowid='a0ab07fa-fe40-4eb5-bdef-5b505defd91a'\n",
    "#table = \"(SELECT workflowid, count(1) as cnt from reach.tasks group by workflowid) AS t\"\n",
    "table = \"(SELECT taskId, title, description, status, submissionid, sectionId from reach.tasks where workflowid='a611477c-41f1-4a9e-9721-d7afeaf55099' and workflowVersion<11) AS t\"\n",
    "\n",
    "tasksDF = spark.read.jdbc(url, table, properties=properties)\n",
    "tasksDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasksDF.select(\"taskId\", col(\"title\").alias(\"taskTitle\"), col(\"description\").alias(\"taskDescription\"), col(\"status\").alias(\"taskStatus\"), \"submissionId\").show(truncate=False)\n",
    "tasksDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskAttribute = tasksDF.select(\"taskId\", col(\"title\").alias(\"taskTitle\"), col(\"description\").alias(\"taskDescription\"), col(\"status\").alias(\"taskStatus\"), col(\"submissionId\").alias(\"taskSubmissionId\"))\n",
    "taskAttribute.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(workflowSchemaPivot.printSchema)\n",
    "print(templatesNonSchemaAttribute.printSchema)\n",
    "print(submissionAttribute.printSchema)\n",
    "print(responseAttribute.printSchema)\n",
    "print(taskAttribute.printSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#workflowSchemaPivot\n",
    "#templatesNonSchemaAttribute\n",
    "#submissionAttribute\n",
    "#responseAttribute\n",
    "#taskAttribute\n",
    "\n",
    "#Join templatesNonSchemaAttribute with submissionAttribute\n",
    "templateSubmissionJoin = templatesNonSchemaAttribute.join(submissionAttribute, (templatesNonSchemaAttribute.workflowid == submissionAttribute.submissionWorkflowId) & (templatesNonSchemaAttribute.workflowversion == submissionAttribute.submissionWorkflowVersion), how=\"inner\")\\\n",
    "    .select(\"workflowid\", \"workflowversion\", \"workflowname\", \"workflowdescription\", \"submissionid\", \"submissionCurrentStep\", \"submissionCreatedTs\", \"submissionLastUpdatedTs\", \"recordId\", \"submissionCountryCode\")\n",
    "\n",
    "templateSubmissionJoin.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, coalesce\n",
    "#Self aggregate response to create a map\n",
    "responseAttributeConcat = responseAttribute.withColumn(\"fieldIdValue\", concat('fieldId', lit(':'), 'responseValue'))\\\n",
    "    .withColumn(\"responseTaskIdCoalesced\", coalesce(\"responseTaskId\", lit(\"\")))\\\n",
    "    .select('responseId', 'responseSubmissionId', 'responseOrder', 'responseTaskIdCoalesced', 'fieldIdValue')\n",
    "#responseAttribute.filter(isnull(\"responseTaskId\")).show(truncate=False)\n",
    "\n",
    "responseAttributeConcat.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "#Aggregate all the fieldIdValue for a combination of submissionId, taskId into a single record\n",
    "responseAggregateRecord = responseAttributeConcat.limit(20).groupBy(\"responseSubmissionId\", \"responseTaskIdCoalesced\")\\\n",
    "                            .agg(collect_set(\"fieldIdValue\").alias(\"fieldIdValues\"))\n",
    "                        \n",
    "responseAggregateRecord.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join templateSubmissionJoin with responseAggregateRecord\n",
    "submissionResponseJoin = templateSubmissionJoin.join(responseAggregateRecord, (templateSubmissionJoin.submissionid == responseAggregateRecord.responseSubmissionId), how='inner')\\\n",
    "    .select(col(\"workflowId\").alias('submissionWorkflowId'), col(\"workflowVersion\").alias('submissionWorkflowVersion'), \"submissionid\", \"responseTaskIdCoalesced\", \"fieldIdValues\")\n",
    "\n",
    "submissionResponseJoin.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to flatten the table for following 3 columns\n",
    "#4f842804-7da0-4a18-afb3-696c7b1a0991 > dyn_inspector39s_contact_information_inspector_phone_number\n",
    "#57c50f5e-df29-4a11-af9d-06e29ab11747 > dyn_assignment_notes_insufficient_paperwork\n",
    "#b98305e1-e911-4a98-9541-df6094937a75 > dyn_additional_details_contact_type\n",
    "\n",
    "#Need to figure out the column names for following fieldIds\n",
    "workflowTemplateSchema.filter(col(\"fieldid\").isin('4f842804-7da0-4a18-afb3-696c7b1a0991','57c50f5e-df29-4a11-af9d-06e29ab11747','b98305e1-e911-4a98-9541-df6094937a75'))\\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###*************************DEBUG*************************###\n",
    "workflowSchemaPivot.select(\"workflowid\", \"workflowversion\", \"dyn_inspector39s_contact_information_inspector_phone_number\", \"dyn_assignment_notes_insufficient_paperwork\", \"dyn_additional_details_contact_type\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###*************************DEBUG*************************###\n",
    "for elem in workflowSchemaPivot.schema.names:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###*************************DEBUG*************************###\n",
    "debugDF01 = submissionResponseJoin.join(workflowSchemaPivot, (submissionResponseJoin.submissionWorkflowId == workflowSchemaPivot.workflowid) & (submissionResponseJoin.submissionWorkflowVersion == workflowSchemaPivot.workflowversion), how='inner')\\\n",
    "    .drop('submissionWorkflowId')\\\n",
    "    .drop('submissionWorkflowVersion')\n",
    "    \n",
    "\n",
    "debugDF01.select('workflowId', 'workflowVersion','submissionId', 'responseTaskIdCoalesced', 'fieldIdValues', 'dyn_inspector39s_contact_information_inspector_phone_number', 'dyn_assignment_notes_insufficient_paperwork', 'dyn_additional_details_contact_type')\\\n",
    "    .show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map the fieldId value to the respective column\n",
    "def search_and_return(fieldId, listKeyValue):\n",
    "    returnValue = ''\n",
    "    for keyValue in listKeyValue:\n",
    "        key = keyValue.split(':')[0]\n",
    "        value = keyValue.split(':')[1]\n",
    "        if str(fieldId) == key:\n",
    "            returnValue = value\n",
    "    return returnValue\n",
    "    \n",
    "\n",
    "\n",
    "sampleKeyValueList = ['12:a', '11:b', '13:c']\n",
    "print(search_and_return(13, sampleKeyValueList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a UDF out of the function\n",
    "search_and_return_udf = udf(search_and_return, StringType())\n",
    "\n",
    "debugDF01.withColumn(\"dyn_inspector39s_contact_information_inspector_phone_number\", search_and_return_udf(col('dyn_inspector39s_contact_information_inspector_phone_number'), 'fieldIdValues'))\\\n",
    "    .withColumn(\"dyn_assignment_notes_insufficient_paperwork\", search_and_return_udf(col('dyn_assignment_notes_insufficient_paperwork'), 'fieldIdValues'))\\\n",
    "    .withColumn(\"dyn_additional_details_contact_type\", search_and_return_udf(col('dyn_additional_details_contact_type'), 'fieldIdValues'))\\\n",
    "    .select('workflowid', 'workflowversion', 'submissionId', 'responseTaskIdCoalesced', 'dyn_inspector39s_contact_information_inspector_phone_number', 'dyn_assignment_notes_insufficient_paperwork', 'dyn_additional_details_contact_type')\\\n",
    "    .show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the replace process automated\n",
    "workflowSchemaDFColumnList = workflowSchemaPivot.schema.names\n",
    "dynamicColumnList = list(filter(lambda x: 'dyn_' in x, workflowSchemaDFColumnList))\n",
    "\n",
    "for colName in dynamicColumnList:\n",
    "    print(colName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debugDF01.withColumn(F.col('dyn_inspector39s_contact_information_inspector_phone_number'), search_and_return_udf(F.col('dyn_inspector39s_contact_information_inspector_phone_number'), 'fieldIdValues'))\\\n",
    "debugDF03 = debugDF01\n",
    "\n",
    "for rName in dynamicColumnList:\n",
    "    debugDF03 = debugDF03.withColumn(rName, search_and_return_udf(col(rName), 'fieldIdValues'))\n",
    "#debugDF03 = debugDF03.withColumn(F.col('dyn_inspector39s_contact_information_inspector_phone_number'), search_and_return_udf(F.col('dyn_inspector39s_contact_information_inspector_phone_number'), 'fieldIdValues'))\n",
    "#debugDF03 = debugDF03.withColumn(F.col('dyn_inspector39s_contact_information_inspector_phone_number'), search_and_return_udf(F.col('dyn_inspector39s_contact_information_inspector_phone_number'), 'fieldIdValues'))\n",
    "\n",
    "#debugDF03 = debugDF03.withColumn('workflowId', lit('workflowId'))\n",
    "#debugDF03 = debugDF03.withColumn('workflowId', col('workflowId'))\n",
    "\n",
    "#debugDF03.withColumn('dyn_inspector39s_contact_information_inspector_phone_number', search_and_return_udf(F.col('dyn_inspector39s_contact_information_inspector_phone_number'), 'fieldIdValues'))\\\n",
    "debugDF03.select('workflowid', 'workflowversion', 'submissionId', 'responseTaskIdCoalesced', 'dyn_inspector39s_contact_information_inspector_phone_number', 'dyn_assignment_notes_insufficient_paperwork', 'dyn_additional_details_contact_type')\\\n",
    "    .show(30, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "orig_nbformat": 2,
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
