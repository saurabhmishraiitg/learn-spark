{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Start the kernel specific to SPARK\n",
        "\n",
        "* conda env list\n",
        "* conda activate spark\n",
        "* in visual code IDE, select the environment as python-spark\n",
        "* check that in the selected environment pyspark is installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 show pyspark\n",
        "!pip3 show findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The following line help the Jupyter program to find the Spark binaries to run the job\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample program to validate pySpark library is available\n",
        "import pyspark\n",
        "sc = pyspark.SparkContext('local[*]')\n",
        "\n",
        "txt = sc.textFile('file:////Users/sxxx/github/spark-scala/README.md')\n",
        "print(txt.count())\n",
        "\n",
        "python_lines = txt.filter(lambda line: 'python' in line.lower())\n",
        "print(python_lines.count())\n",
        "\n",
        "\n",
        "#big_list = range(10000)\n",
        "#>>> rdd = sc.parallelize(big_list, 2)\n",
        "#>>> odds = rdd.filter(lambda x: x % 2 != 0)\n",
        "#>>> odds.take(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Another random code snippet to check if the Spark session is still alive after the previous cell execution\n",
        "big_list = range(10000)\n",
        "rdd = sc.parallelize(big_list, 2)\n",
        "odds = rdd.filter(lambda x: x % 2 != 0)\n",
        "odds.take(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the CCM properties file\n",
        "from os.path import expanduser\n",
        "home = expanduser(\"~\")\n",
        "\n",
        "separator = \"=\"\n",
        "keys = {}\n",
        "\n",
        "# I named your file conf and stored it \n",
        "# in the same directory as the script\n",
        "\n",
        "with open(home+'/nexus.prop') as f:\n",
        "\n",
        "    for line in f:\n",
        "        if separator in line:\n",
        "\n",
        "            # Find the name and value by splitting the string\n",
        "            name, value = line.split(separator, 1)\n",
        "\n",
        "            # Assign key value pair to dict\n",
        "            # strip() removes white space from the ends of strings\n",
        "            keys[name.strip()] = value.strip()\n",
        "\n",
        "#print(keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This section handles adding JDBC driver to the PYSPARK shell\n",
        "import os\n",
        "jdbc_connector_mysql=keys[\"jdbc-connector-mysql\"]\n",
        "\n",
        "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"--jars file://{jdbc_connector_mysql} pyspark-shell\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create sparksession instance\n",
        "\n",
        "spark.stop()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch 1 workflow records from the templates table\n",
        "#database = 'reach'\n",
        "#url='jdbc:mysql://{}/{}'.format(host, database)\n",
        "#table = 'reach.tasks'\n",
        "\n",
        "table = \"(SELECT workflowId, REPLACE(CONVERT(REPLACE(workflow,'&amp;','&') using utf8),'','') as workflow FROM reach.templates LIMIT 3) AS t\"\n",
        "\n",
        "url = keys[\"reach-dev-url\"]\n",
        "#query='select taskid from tasks limit 10'\n",
        "user = keys[\"reach-dev-userid\"]\n",
        "password = keys[\"reach-dev-password\"]\n",
        "\n",
        "properties = {\n",
        "    'user': user,\n",
        "    'password': password,\n",
        "    'driver': 'com.mysql.jdbc.Driver',\n",
        "    #'query': 'select taskid from tasks limit 10',\n",
        "    'fetchsize': '10'\n",
        "}\n",
        "\n",
        "df = spark.read.jdbc(url, table, properties=properties)\n",
        "#df = sqlContext.read.jdbc(url, query, properties=properties)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Parse the json column\n",
        "df.select(\"workflow\").show()"
      ]
    }
  ],
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "orig_nbformat": 2,
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
