{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hadoop File Formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Test out various hadoop data formats\n",
        "#Compare their compression ratio\n",
        "#Compare their read/write times\n",
        "#Compare file split counts/splittability\n",
        "\n",
        "##Formats\n",
        "    #ORC\n",
        "    #Delimited\n",
        "    #Serialized\n",
        "    #AVRO\n",
        "    #Parquet\n",
        "\n",
        "##Compression\n",
        "    #Snappy\n",
        "    #GZIP\n",
        "    #Zlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 show pyspark\n",
        "!pip3 show findspark\n",
        "\n",
        "# The following line help the Jupyter program to find the Spark binaries to run the job\n",
        "import findspark\n",
        "findspark.init()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the CCM properties file\n",
        "from os.path import expanduser\n",
        "home = expanduser(\"~\")\n",
        "\n",
        "separator = \"=\"\n",
        "keys = {}\n",
        "\n",
        "# I named your file conf and stored it \n",
        "# in the same directory as the script\n",
        "\n",
        "with open(home+'/nexus.prop') as f:\n",
        "\n",
        "    for line in f:\n",
        "        if separator in line:\n",
        "\n",
        "            # Find the name and value by splitting the string\n",
        "            name, value = line.split(separator, 1)\n",
        "\n",
        "            # Assign key value pair to dict\n",
        "            # strip() removes white space from the ends of strings\n",
        "            keys[name.strip()] = value.strip()\n",
        "\n",
        "#print(keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Create sparksession instance\n",
        "\n",
        "#spark.stop()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Read a decent sized file from local and try to save that in multiple formats\n",
        "tmp_dir=keys[\"tmp-dir\"]\n",
        "tmp_output_dir=tmp_dir+\"/sample-output/file-format\"\n",
        "tmp_input_dir=tmp_dir+\"/sample-dataset/taxi-trip-data/yellow_tripdata_2016-01.csv.small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Load the data in a dataframe\n",
        "sampleDataDF = spark.read.option('header','true').csv(tmp_input_dir)\n",
        "sampleDataDF.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampleDataDF.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save as a Snappy compressed CSV\n",
        "sampleDataDF.write.option('header','true').mode('overwrite').csv(tmp_output_dir+\"/csv01\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save as a Snappy compressed Parquet\n",
        "sampleDataDF.write.option('header','true').mode('overwrite').parquet(tmp_output_dir+\"/parquet01\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save as a Snappy compressed ORC\n",
        "sampleDataDF.write.option('header','true').mode('overwrite').orc(tmp_output_dir+\"/orc01\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save as a Snappy compressed AVRO\n",
        "sampleDataDF.write.option('header','true').mode('overwrite').avro(tmp_output_dir+\"/avro01\")"
      ]
    }
  ],
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3.7.5 64-bit ('spark': conda)",
      "name": "python37564bitsparkconda4e3b4d5f3c4d4cadbf8fd6a78e253cc0"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "version": "3.7.5-final"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "orig_nbformat": 2,
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
