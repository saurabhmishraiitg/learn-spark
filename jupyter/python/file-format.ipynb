{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop File Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test out various hadoop data formats\n",
    "#Compare their compression ratio\n",
    "#Compare their read/write times\n",
    "#Compare file split counts/splittability\n",
    "\n",
    "##Formats\n",
    "    #ORC\n",
    "    #Delimited\n",
    "    #Serialized\n",
    "    #AVRO\n",
    "    #Parquet\n",
    "\n",
    "##Compression\n",
    "    #Snappy\n",
    "    #GZIP\n",
    "    #Zlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 2.4.4\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /usr/local/lib/python3.7/site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "Name: findspark\n",
      "Version: 1.3.0\n",
      "Summary: Find pyspark to make it importable.\n",
      "Home-page: https://github.com/minrk/findspark\n",
      "Author: Min RK\n",
      "Author-email: benjaminrk@gmail.com\n",
      "License: BSD (3-clause)\n",
      "Location: /usr/local/lib/python3.7/site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip3 show pyspark\n",
    "!pip3 show findspark\n",
    "\n",
    "# The following line help the Jupyter program to find the Spark binaries to run the job\n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CCM properties file\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "separator = \"=\"\n",
    "keys = {}\n",
    "\n",
    "# I named your file conf and stored it \n",
    "# in the same directory as the script\n",
    "\n",
    "with open(home+'/nexus.prop') as f:\n",
    "\n",
    "    for line in f:\n",
    "        if separator in line:\n",
    "\n",
    "            # Find the name and value by splitting the string\n",
    "            name, value = line.split(separator, 1)\n",
    "\n",
    "            # Assign key value pair to dict\n",
    "            # strip() removes white space from the ends of strings\n",
    "            keys[name.strip()] = value.strip()\n",
    "\n",
    "#print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sparksession instance\n",
    "\n",
    "#spark.stop()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read a decent sized file from local and try to save that in multiple formats\n",
    "tmp_dir=keys[\"tmp-dir\"]\n",
    "tmp_output_dir=tmp_dir+\"/sample-output/file-format\"\n",
    "tmp_input_dir=tmp_dir+\"/sample-dataset/taxi-trip-data/yellow_tripdata_2016-01.csv.small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- pickup_longitude: string (nullable = true)\n",
      " |-- pickup_latitude: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_longitude: string (nullable = true)\n",
      " |-- dropoff_latitude: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the data in a dataframe\n",
    "sampleDataDF = spark.read.option('header','true').csv(tmp_input_dir)\n",
    "sampleDataDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|   pickup_longitude|   pickup_latitude|RatecodeID|store_and_fwd_flag|  dropoff_longitude|  dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|       2| 2016-01-01 00:00:00|  2016-01-01 00:00:00|              2|         1.10|-73.990371704101563|40.734695434570313|         1|                 N|-73.981842041015625|40.732406616210937|           2|        7.5|  0.5|    0.5|         0|           0|                  0.3|         8.8|\n",
      "|       2| 2016-01-01 00:00:00|  2016-01-01 00:00:00|              5|         4.90|-73.980781555175781|40.729911804199219|         1|                 N|-73.944473266601563|40.716678619384766|           1|         18|  0.5|    0.5|         0|           0|                  0.3|        19.3|\n",
      "|       2| 2016-01-01 00:00:00|  2016-01-01 00:00:00|              1|        10.54|-73.984550476074219|  40.6795654296875|         1|                 N|-73.950271606445313|40.788925170898438|           1|         33|  0.5|    0.5|         0|           0|                  0.3|        34.3|\n",
      "+--------+--------------------+---------------------+---------------+-------------+-------------------+------------------+----------+------------------+-------------------+------------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleDataDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save as a Snappy compressed CSV\n",
    "sampleDataDF.write.option('header','true').mode('overwrite').csv(tmp_output_dir+\"/csv01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save as a Snappy compressed Parquet\n",
    "sampleDataDF.write.option('header','true').mode('overwrite').parquet(tmp_output_dir+\"/parquet01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save as a Snappy compressed ORC\n",
    "sampleDataDF.write.option('header','true').mode('overwrite').orc(tmp_output_dir+\"/orc01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrameWriter' object has no attribute 'avro'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-95811708495d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Save as a Snappy compressed AVRO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msampleDataDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_output_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/avro01\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameWriter' object has no attribute 'avro'"
     ]
    }
   ],
   "source": [
    "#Save as a Snappy compressed AVRO\n",
    "sampleDataDF.write.option('header','true').mode('overwrite').avro(tmp_output_dir+\"/avro01\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('spark': conda)",
   "name": "python37564bitsparkconda4e3b4d5f3c4d4cadbf8fd6a78e253cc0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "version": "3.7.5-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "orig_nbformat": 2,
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
